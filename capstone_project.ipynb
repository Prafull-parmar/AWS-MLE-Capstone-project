{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6ea539b",
   "metadata": {},
   "source": [
    "# AWS Machine Learning Engineer Nano Degree Capstone Project "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af16e72",
   "metadata": {},
   "source": [
    "## Plants Disease Detection Using Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe33740",
   "metadata": {},
   "source": [
    "### Project  Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cb27ae",
   "metadata": {},
   "source": [
    "Plant diseases are one of the major factors responsible for substantial losses in yield of\n",
    "plants, leading to huge economic losses. According to a study by the Associated Chambers\n",
    "of Commerce and Industry of India, annual crop losses due to diseases and pest’s amount to\n",
    "Rs.50,000 crore in India alone, which is significant in a country where the\n",
    "farmers are responsible for feeding a population of close to 1.3 billion people. The value of\n",
    "plant science is therefore huge.</br>\n",
    "Accurate identification and diagnosis of plant diseases are very important in the era of\n",
    "climate change and globalization for food security. Accurate and early identification of plant\n",
    "diseases could help in the prevention of spread of invasive pests/pathogens. In addition, for an\n",
    "efficient and economical management of plant diseases accurate, sensitive and specific\n",
    "diagnosis is necessary.</br>\n",
    "The growth of GPU’s ( Graphical Processing Units ) has aided academics and business\n",
    "in the advancement of Deep Learning methods, allowing them to explore deeper and more\n",
    "sophisticated Neural Networks. Using concepts of Image Classification and Transfer\n",
    "Learning we could train a Deep Learning model to categorize Plant leaf’s images to predict\n",
    "whether the plant is healthy or has any diseases. This could help in the early detection of any\n",
    "diseases in plants and could help take preventive measures to prevent huge crop losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5e3dd0",
   "metadata": {},
   "source": [
    "### Data Prepation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ec7c37",
   "metadata": {},
   "source": [
    "####  Installing Libraries\n",
    "* We will be using **split-folders** to split our dataset into train, val and test sets.\n",
    "* **tqdm** will help give us a visual status of the progress while copying folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a64eb7f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: split-folders in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (0.4.3)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (4.61.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install split-folders tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eed4025",
   "metadata": {},
   "source": [
    "#### Download the zipped dataset file from S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "525a4dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-us-east-1-970845818811/CapstoneProposal.zip to ./CapstoneProposal.zip\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp s3://sagemaker-us-east-1-970845818811/CapstoneProposal.zip ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "364be054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets write a small utility functions to unzip our folder's contents.\n",
    "import zipfile\n",
    "\n",
    "# Function below unzips the archive to the local directory. \n",
    "def unzip_data(input_data_path):\n",
    "    with zipfile.ZipFile(input_data_path, 'r') as input_data_zip:\n",
    "        input_data_zip.extractall('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4aa3a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "zipped_filename = \"CapstoneProposal.zip\"\n",
    "unzip_data(zipped_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae9defb",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder_path = \"./CapstoneProposal/dataset/Plant_leave_diseases_dataset_with_augmentation\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd312a3e",
   "metadata": {},
   "source": [
    "##### Quick Overview of the plant diesease dataset.\n",
    "The total plant disease dataset of that will be used for this project consists of **9644 images** . All the images\n",
    "vary in dimensions, they are not standardized, and they are all coloured images. So the model\n",
    "will be trained on the above 9 plant image classes, for our use-case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de361790",
   "metadata": {},
   "source": [
    "#### Splitting Dataset into Train, Validation and Test Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ddbfa4",
   "metadata": {},
   "source": [
    "The Dataset consists of **9 classes** and the dataset is more or less **balanced**. Thus we can split the dataset into train , validation and test sets in the ratio/proportion of \n",
    "**80:10:10**. Meaning 80% training dataset, 10% validation dataset and 10% test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d643d6b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 9644 files [00:01, 7039.60 files/s]\n"
     ]
    }
   ],
   "source": [
    "import splitfolders  # or import split_folders\n",
    "# Split with a ratio.\n",
    "splitfolders.ratio(input_folder_path, output=\"plant_disease_dataset\", seed=1357, ratio=(.8, .1, .1), group_prefix=None) # default valuesb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de06029",
   "metadata": {},
   "source": [
    "#### Uploading the split datset onto S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dcd03e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp plant_disease_dataset s3://sagemaker-us-east-1-970845818811/plant_disease_dataset --recursive > /dev/null "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2627c2d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bac2c50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b153076",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
